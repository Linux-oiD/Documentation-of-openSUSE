<?xml version="1.0"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.libvirt.admin">
 <title>Administrating &vmguest;s</title>
 <para/>
 <sect1 id="sec.libvirt.admin.migrate">
  <title>Migrating &vmguest;s</title>

  <para>
   One of the major advantages of virtualization is the fact that &vmguest;s
   are portable. When a &vmhost; needs to go down for maintenance, or when
   the host gets overloaded, the guests can easily be moved to another
   &vmhost;. &kvm; and &xen; even support <quote>live</quote> migrations
   during which the &vmguest; is constantly available (live migrations).
  </para>

  <para>
   In order to successfully migrate a &vmguest; to another &vmhost;, the
   following requirements need to be met:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Host and target must have same processor manufacturer (Intel or AMD).
    </para>
   </listitem>
<!-- 32 bit guests are not supported by Novell -->
   <listitem os="osuse">
    <para>
     &vmguest;s running a 64-bit operating system can only be migrated to a
     64-bit host (whereas 32-bit &vmguest;s can be moved to a 64 or 32-bit
     host).
    </para>
   </listitem>
   <listitem>
    <para>
     Storage devices must be accessible from both machines (for example, via
     NFS or iSCSI) and must be configured as a storage pool on both machines
     (see <xref linkend="sec.libvirt.stornet.storage"/> for more
     information). This is also true for CD-ROM or floppy images that are
     connected during the move (however, you may disconnect them prior to
     the move as described in
     <xref linkend="sec.libvirt.config.cdrom.media_change"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     &libvirtd; needs to run on both &vmhost;s and you must be able to open
     a remote &libvirt; connection between the target and the source host
     (or vice versa). Refer to
     <xref
      linkend="sec.libvirt.connect.remote"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     If a firewall is running on the target host ports need to be opened to
     allow the migration. If you do not specify a port during the migration
     process, &libvirt; chooses one from the range 49152:49215. Make sure
     that either this range (recommended) or a dedicated port of your choice
     is opened in the firewall on the <emphasis>target host</emphasis>.
    </para>
   </listitem>
   <listitem>
    <para>
     Host and target machine should be in the same subnet on the network,
     otherwise networking will not work after the migration.
    </para>
   </listitem>
   <listitem>
    <para>
     No running or paused &vmguest; with the same name must exist on the
     target host. If a shut down machine with the same name exists, its
     configuration will be overwritten.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 id="sec.libvirt.admin.migrate.virtmanager">
   <title>Migrating with <command>virt-manager</command></title>
   <para>
    When using the &vmm; to migrate &vmguest;s, it does not matter on which
    machine it is started. You can start &vmm; on the source or the target
    host or even on a third host. In the latter case you need to be able to
    open remote connections to both the target and the source host.
   </para>
   <procedure>
    <step>
     <para>
      Start &vmm; and establish a connection to the target or the source
      host. If the &vmm; was started neither on the target nor the source
      host, connections to both hosts need to be opened.
     </para>
    </step>
    <step>
     <para>
      Right-click on the &vmguest; that is to be migrated and choose
      <guimenu>Migrate</guimenu>. Make sure the guest is running or paused -
      it is not possible to migrate guests that are shut off.
     </para>
    </step>
    <step>
     <para>
      Choose a <guimenu>New Host</guimenu> for the &vmguest;. If the desired
      target host does not show up, make sure a connection to this host has
      been established.
     </para>
     <para>
      By default, a <quote>live</quote> migration is performed. If you
      prefer an <quote>offline</quote> migration where the &vmguest; is
      paused during the migration, tick <guimenu>Migrate offline</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Migrate</guimenu> to start a migration with the default
      port and bandwidth.
     </para>
     <para>
      In order to change these defaults, make the advanced options available
      by clicking the triangle at <guimenu>Advanced Options</guimenu>. Here
      you can enter the target host's <guimenu>Address</guimenu> (IP address
      or hostname), a port and the bandwidth in megabit per second (Mbps).
      If you specify a <guimenu>Port</guimenu>, you must also specify an
      <guimenu>Address</guimenu>; the <guimenu>Bandwidth</guimenu> is
      optional.
     </para>
    </step>
    <step>
     <para>
      Once the migration is complete, the <guimenu>Migrate</guimenu> window
      closes and the &vmguest; is now listed on the new host in the &vmm;
      Window. The original &vmguest; will still be available on the target
      host (in state shut off).
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.libvirt.admin.migrate.virsh">
   <title>Migrating with <command>virsh</command></title>
   <para>
    To migrate a &vmguest; with <command>virsh
    <option>migrate</option></command>, you need to have direct or remote
    shell access to the &vmhost;, because the command needs to be run on the
    host. Basically the migration command looks like this
   </para>
<screen>virsh migrate [OPTIONS] <replaceable>VM_ID_or_NAME</replaceable> <replaceable>CONNECTION URI</replaceable> [--migrateuri tcp://<replaceable>REMOTE_HOST:PORT</replaceable>]</screen>
   <para>
    The most important options are listed below. See <command>virsh help
    migrate</command> for a full list.
   </para>
   <variablelist>
    <varlistentry>
     <term><option>--live</option>
     </term>
     <listitem>
      <para>
       Does a live migration. If not specified, an offline migration where
       the &vmguest; is paused during the migration, will be performed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--suspend</option>
     </term>
     <listitem>
      <para>
       Does an offline migration and does not restart the &vmguest; on the
       target host.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--persistent</option>
     </term>
     <listitem>
      <para>
       By default a migrated &vmguest; will be migrated transient, so its
       configuration is automatically deleted on the target host if it is
       shut down. Use this switch to make the migration persistent.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>--undefinesource</option>
     </term>
     <listitem>
      <para>
       When specified, the &vmguest; definition on the source host will be
       deleted after a successful migration.
      </para>
      <warning>
       <para>
        It is recommended to use <option>--persistent</option> when
        specifying <option>--undefinesource</option>, otherwise the
        &vmguest; configuration will be lost on both hosts when the guest is
        shut down on the target host.
       </para>
      </warning>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following examples use &wsIVname; as the source system and &wsIname;
    as the target system, the &vmguest;'s name is
    <literal>opensuse11</literal> with Id <literal>37</literal>.
   </para>
<screen># offline migration with default parameters
virsh migrate 37 qemu+ssh://root@&wsIname;/system

# transient live migration with default parameters
virsh migrate --live opensuse11 qemu+ssh://root@&wsIname;/system

# persistent live migration; delete VM definition on source
virsh migrate --live --persistent --undefinesource 37 \
qemu+tls://root@&wsIname;/system

# offline migration using port 49152
virsh migrate opensuse11 qemu+ssh://root@&wsIname;/system \
--migrateuri tcp://@&wsIname;:49152</screen>
  </sect2>
 </sect1>
 <sect1 id="cha.libvirt.admin.monitor">
  <title>Monitoring</title>

  <para/>

  <sect2 id="cha.libvirt.admin.monitor.virt-manager">
   <title>Monitoring with &vmm;</title>
   <para>
    After starting &vmm; and connecting to the &vmhost;, a CPU usage graph
    of all the running guests is displayed.
   </para>
   <para>
    It is also possible to get information about disk and network usage with
    this tool, however, you must first activate this in the
    <guimenu>Preferences</guimenu>:
   </para>
   <procedure>
    <step>
     <para>
      Run <command>virt-manager</command>.
     </para>
    </step>
    <step>
     <para>
      Select <menuchoice><guimenu>Edit</guimenu>
      <guimenu>Preferences</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Change the tab from <guimenu>General</guimenu> to
      <guimenu>Stats</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Activate the check boxes for <guimenu>Disk I/O</guimenu> and
      <guimenu>Network I/O</guimenu>.
     </para>
    </step>
    <step>
     <para>
      If desired, also change the update interval or the number of samples
      that are kept in the history.
     </para>
    </step>
    <step>
     <para>
      Close the <guimenu>Preferences</guimenu> dialog.
     </para>
    </step>
    <step>
     <para>
      Activate the graphs that should be displayed under <menuchoice>
      <guimenu>View</guimenu> <guimenu>Graph</guimenu> </menuchoice>.
     </para>
    </step>
   </procedure>
   <para>
    Afterwards, the disk and network statistics are also displayed in the
    main window of the &vmm;.
   </para>
   <para>
    More precise data is available from the VNC window. Open a VNC window as
    described in <xref linkend="sec.libvirt.managing.vnc"/>. Choose
    <guimenu>Details</guimenu> from the toolbar or the
    <guimenu>View</guimenu> menu. The statistics are displayed from the
    <guimenu>Performance</guimenu> entry of the left-hand tree menu.
   </para>
  </sect2>

  <sect2>
   <title>Monitoring with <command>kvm_stat</command></title>
   <para>
    <command>kvm_stat</command> can be used to trace &kvm; performance
    events. It monitors <filename>/sys/kernel/debug/kvm</filename>, so it
    needs the debugfs to be mounted. On &productname; it should be mounted
    by default. In case it is not mounted, use the following command:
   </para>
<screen>mount -t debugfs none /sys/kernel/debug</screen>
   <para>
    <command>kvm_stat</command> can be used in three different modes:
   </para>
<screen>kvm_stat                    # update in 1 second intervals
kvm_stat -1                 # 1 second snapshot
kvm_stat -l > kvmstats.log  # update in 1 second intervals in log format
                            # can be imported to a spreadsheet</screen>
   <example>
    <title>Typical Output of <command>kvm_stat</command></title>
<screen>kvm statistics

 efer_reload                  0       0
 exits                 11378946  218130
 fpu_reload               62144     152
 halt_exits              414866     100
 halt_wakeup             260358      50
 host_state_reload       539650     249
 hypercalls                   0       0
 insn_emulation         6227331  173067
 insn_emulation_fail          0       0
 invlpg                  227281      47
 io_exits                113148      18
 irq_exits               168474     127
 irq_injections          482804     123
 irq_window               51270      18
 largepages                   0       0
 mmio_exits                6925       0
 mmu_cache_miss           71820      19
 mmu_flooded              35420       9
 mmu_pde_zapped           64763      20
 mmu_pte_updated              0       0
 mmu_pte_write           213782      29
 mmu_recycled                 0       0
 mmu_shadow_zapped       128690      17
 mmu_unsync                  46      -1
 nmi_injections               0       0
 nmi_window                   0       0
 pf_fixed               1553821     857
 pf_guest               1018832     562
 remote_tlb_flush        174007      37
 request_irq                  0       0
 signal_exits                 0       0
 tlb_flush               394182     148</screen>
   </example>
   <para>
    See
    <ulink
     url="http://clalance.blogspot.com/2009/01/kvm-performance-tools.html"/>
    for further information on how to interpret these values.
   </para>
  </sect2>
 </sect1>
<!--

 <sect1 id="cha.kvm.admin.resize">
  <title>Resizing Virtual Disks</title>
  <para>
   
  </para>
 </sect1>

 <sect1>
  <title>Updating Virtual Machines</title>
  <para></para>
 </sect1>
-->
</chapter>
