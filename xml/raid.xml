<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1 PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<sect1 id="sec.yast2.system.raid">
 <title>Soft RAID Configuration</title><indexterm>

 <primary>&yast;</primary>

 <secondary>RAID</secondary></indexterm><indexterm>

 <primary>RAID</primary>

 <secondary>&yast;</secondary></indexterm><indexterm>

 <primary>soft RAID</primary>

 <see>RAID</see></indexterm>

<!-- TODO: don't know what todo with this doccomment:
 In section 2.3.1 RAID Levels to streamline the documentation you may
 want to remove the paragraphs on RAID 2 and RAID 3 and on RAID 4 and
 add these RAID levels to the list of other RAID levels under the
 heading below Other RAID Levels. Suggesting this because in the next
 section on configurating RAID only configuring RAID levels 0, 1, 5
 are mentioned. In addition, the paragraph on RAID 4 mentions a
 bottleneck for write access to the parity disk. This problem has
 typically been avoided in the "proprietary implementations created by
 hardware vendor" that you refer to under the Other RAID levels
 heading, for example, the Network Appliance implementation of RAID 4.

 You may also want to do the same with RAID 6.

 In section 2.3.2 Soft RAID Configuration with YaST when "Checking
 Persistent Superblock ensures" is mentioned it may be helpful to
 display in a figure where to check this option. -->

 <para>
  The purpose of RAID (redundant array of independent disks) is to combine
  several hard disk partitions into one large <emphasis>virtual</emphasis>
  hard disk to optimize performance and/or data security. Most RAID
  controllers use the SCSI protocol because it can address a larger number
  of hard disks in a more effective way than the IDE protocol. It is also
  more suitable for the parallel command processing. There are some RAID
  controllers that support IDE or SATA hard disks. Soft RAID provides the
  advantages of RAID systems without the additional cost of hardware RAID
  controllers. However, this requires some CPU time and has memory
  requirements that make it unsuitable for high performance computers.
 </para>

 <para>
  With &productnamereg; , you can combine several hard disks into one soft
  RAID system. RAID implies several strategies for combining several hard
  disks in a RAID system, each with different goals, advantages, and
  characteristics. These variations are commonly known as <emphasis>RAID
  levels</emphasis>.
 </para>

 <para>
  Common RAID levels are:
 </para>

 <variablelist>
  <varlistentry>
   <term>RAID&nbsp;0</term>
   <listitem>
    <para>
     This level improves the performance of your data access by spreading
     out blocks of each file across multiple disk drives. Actually, this is
     not really a RAID, because it does not provide data backup, but the
     name <emphasis>RAID&nbsp;0</emphasis> for this type of system is
     commonly used. With RAID&nbsp;0, two or more hard disks are pooled
     together. Performance is enhanced, but the RAID system is destroyed and
     your data lost if even one hard disk fails.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;1</term>
   <listitem>
    <para>
     This level provides adequate security for your data, because the data
     is copied to another hard disk 1:1. This is known as <emphasis>hard
     disk mirroring</emphasis>. If one disk is destroyed, a copy of its
     contents is available on the other one. All disks but one could be
     damaged without endangering your data. However, if the damage is not
     detected, the damaged data can be mirrored to the undamaged disk. This
     could result in the same loss of data. The writing performance suffers
     in the copying process compared to using single disk access (10 to 20 %
     slower), but read access is significantly faster in comparison to any
     one of the normal physical hard disks. The reason is that the duplicate
     data can be parallel-scanned. Generally it can be said that
     Level&nbsp;1 provides nearly twice the read transfer rate of single
     disks and almost the same write transfer rate as single disks.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;2 and RAID&nbsp;3</term>
   <listitem>
    <para>
     These are not typical RAID implementations. Level&nbsp;2 stripes data
     at the bit level rather than the block level. Level&nbsp;3 provides
     byte-level striping with a dedicated parity disk, and cannot service
     simultaneous multiple requests. These levels are rarely used.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;4</term>
   <listitem>
    <para>
     Level&nbsp;4 provides block-level striping just like Level&nbsp;0
     combined with a dedicated parity disk. In the case of data disk
     failure, the parity data is used to create a replacement disk. However,
     the parallel disk may create a bottleneck for write access.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term>RAID&nbsp;5</term>
   <listitem>
    <para>
     RAID&nbsp;5 is an optimized compromise between Level&nbsp;0 and
     Level&nbsp;1, in terms of performance and redundancy. The hard disk
     space equals the number of disks used minus one. The data is
     distributed over the hard disks as with RAID&nbsp;0. <emphasis>Parity
     blocks</emphasis>, created on one of the partitions, exist for security
     reasons. They are linked to each other with XOR, enabling the contents
     to be reconstructed by the corresponding parity block in case of system
     failure. With RAID&nbsp;5, no more than one hard disk can fail at the
     same time. If one hard disk fails, it must be replaced as soon as
     possible to avoid the risk of losing data.
    </para>
   </listitem>
  </varlistentry>
<!-- bg: this is not a common raid level. Merging to SLE_100
   <varlistentry>
    <term>RAID&nbsp;6</term>
    <listitem>
     <para>
      To further increase the reliability of the RAID system, it is
      possible to use RAID&nbsp;6. In this level, even if two disks 
      fail, the array still can be reconstructed. With
      RAID&nbsp;6, at least 4 hard disks are needed to run the array.
      Note that when running as software raid, this
      configuration needs a considerable amount of CPU time and
      memory.
     </para>
    </listitem>
   </varlistentry> -->
  <varlistentry>
   <term>Other RAID Levels</term>
   <listitem>
    <para>
     Several other RAID levels have been developed (RAIDn, RAID&nbsp;10,
     RAID&nbsp;0+1, RAID&nbsp;30, RAID&nbsp;50, etc.), some of them being
     proprietary implementations created by hardware vendors. These levels
     are not very common and therefore are not explained here.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>

 <sect2 id="sec.yast2.system.raid.conf">
  <title>Soft RAID Configuration with &yast;</title>
  <para>
   The &yast; <guimenu>RAID</guimenu> configuration can be reached from the
   &yast; Expert Partitioner, described in
   <xref linkend="sec.yast2.i_y2_part_expert"/>. This partitioning tool
   enables you to edit and delete existing partitions and create new ones to
   be used with soft RAID:
  </para>
  <procedure>
   <step>
    <para>
     Select a hard disk from <guimenu>Hard Disks</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Change to the <guimenu>Partitions</guimenu> tab.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Add</guimenu> and enter the desired size of the raid
     partition on this disk.
    </para>
   </step>
   <step>
    <para>
     Use <guimenu>Do not Format the Partition</guimenu> and change the
     <guimenu>File System ID</guimenu> to <guimenu>0xFD Linux
     RAID</guimenu>. Do not mount this partition.
    </para>
   </step>
   <step>
    <para>
     Repeat this procedure until you have defined all the desired physical
     volumes on the available disks.
    </para>
   </step>
  </procedure>
  <para>
   For RAID&nbsp;0 and RAID&nbsp;1, at least two partitions are
   needed&mdash;for RAID&nbsp;1, usually exactly two and no more. If
   RAID&nbsp;5 is used, at least three partitions are required. It is
   recommended to utilize partitions of the same size only. The RAID
   partitions should be located on different hard disks to decrease the risk
   of losing data if one is defective (RAID&nbsp;1 and 5) and to optimize
   the performance of RAID&nbsp;0. After creating all the partitions to use
   with RAID, click <menuchoice><guimenu>RAID</guimenu><guimenu>Add
   RAID</guimenu></menuchoice> to start the RAID configuration.
  </para>
  <para>
   In the next dialog, choose between RAID levels 0, 1, 5, 6 and 10. Then,
   select all partitions with either the <quote>Linux RAID</quote> or
   <quote>Linux native</quote> type that should be used by the RAID system.
   No swap or DOS partitions are shown.
  </para>
  <figure id="fig.yast2.system.raid.conf">
   <title>RAID Partitions</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="yast2_raid4.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="yast2_raid4.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
  <para>
   To add a previously unassigned partition to the selected RAID volume,
   first click the partition then <guimenu>Add</guimenu>. Assign all
   partitions reserved for RAID. Otherwise, the space on the partition
   remains unused. After assigning all partitions, click
   <guimenu>Next</guimenu> to select the available <guimenu>RAID
   Options</guimenu>.
  </para>
  <para>
   In this last step, set the file system to use as well as encryption and
   the mount point for the RAID volume. After completing the configuration
   with <guimenu>Finish</guimenu>, see the <filename>/dev/md0</filename>
   device and others indicated with <emphasis>RAID</emphasis> in the expert
   partitioner.
  </para>
 </sect2>

 <sect2>
  <title>Troubleshooting</title>
  <para>
   Check the file <filename>/proc/mdstat</filename> to find out whether a
   RAID partition has been damaged. In the event of a system failure, shut
   down your Linux system and replace the defective hard disk with a new one
   partitioned the same way. Then restart your system and enter the command
   <command>mdadm /dev/mdX --add /dev/sdX</command>. Replace 'X' with your
   particular device identifiers. This integrates the hard disk
   automatically into the RAID system and fully reconstructs it.
  </para>
  <para>
   Note that although you can access all data during the rebuild, you may
   encounter some performance issues until the RAID has been fully rebuilt.
  </para>
 </sect2>

<?dbfo-need height="10em"?>

 <sect2>
  <title>For More Information</title>
  <para>
   Configuration instructions and more details for soft RAID can be found in
   the HOWTOs at:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <filename>/usr/share/doc/packages/mdadm/Software-RAID.HOWTO.html</filename>
    </para>
   </listitem>
   <listitem>
    <para>
     <ulink url="http://raid.wiki.kernel.org"/>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Linux RAID mailing lists are available, such as
   <ulink url="http://marc.theaimsgroup.com/?l=linux-raid"/>.
  </para>
 </sect2>
</sect1>
